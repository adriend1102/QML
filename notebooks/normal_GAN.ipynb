{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.animation\n",
    "\n",
    "from time import time\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2139],\n",
       "        [-0.2239],\n",
       "        [-0.2220],\n",
       "        [-0.2134],\n",
       "        [-0.2263],\n",
       "        [-0.2074],\n",
       "        [-0.2290],\n",
       "        [-0.2267],\n",
       "        [-0.2285],\n",
       "        [-0.2136]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Generator_distribution(nn.Module):\n",
    "    def __init__(self,input_dim=1):\n",
    "        # initialize nn Module\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList() \n",
    "        \n",
    "        # architecture\n",
    "        self.layers.append(nn.Linear(input_dim,64))\n",
    "        self.layers.append(nn.LeakyReLU())\n",
    "        self.layers.append(nn.Linear(64,32))\n",
    "        self.layers.append(nn.LeakyReLU())\n",
    "        self.layers.append(nn.Linear(32,1))\n",
    "        \n",
    "    def forward(self,input_tensor):\n",
    "        x = input_tensor\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x\n",
    "    \n",
    "yoyo = Generator_distribution()\n",
    "yoyo.forward(torch.rand(10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4702],\n",
       "        [0.4725],\n",
       "        [0.4712],\n",
       "        [0.4690],\n",
       "        [0.4726],\n",
       "        [0.4718],\n",
       "        [0.4677],\n",
       "        [0.4645],\n",
       "        [0.4704],\n",
       "        [0.4704]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Discriminator_distribution(nn.Module):\n",
    "    def __init__(self,input_dim=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList() \n",
    "        \n",
    "        self.layers.append(nn.Linear(input_dim,64))\n",
    "        self.layers.append(nn.LeakyReLU())\n",
    "        self.layers.append(nn.Linear(64,32))\n",
    "        self.layers.append(nn.LeakyReLU())\n",
    "        self.layers.append(nn.Linear(32,1))\n",
    "        self.layers.append(nn.Sigmoid())\n",
    "    \n",
    "    def forward(self,input_tensor):\n",
    "        x = input_tensor\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "yoyo = Discriminator_distribution(input_dim=1)\n",
    "yoyo.forward(torch.rand(10,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class GAN_distribution():\n",
    "    def __init__(self,batch_size=10,n_epochs=600,lr_generator=0.0001,lr_discriminator=0.0004,from_dist=torch.rand,to_dist=torch.randn):\n",
    "        \n",
    "        # models\n",
    "        self.generator     = Generator_distribution()\n",
    "        self.discriminator = Discriminator_distribution()\n",
    "        \n",
    "        # criterion\n",
    "        self.criterion = nn.BCELoss()\n",
    "        \n",
    "        # optimizers\n",
    "        self.optim_generator     = optim.Adam(self.generator.parameters(),    lr=lr_generator)\n",
    "        self.optim_discriminator = optim.Adam(self.discriminator.parameters(),lr=lr_discriminator)\n",
    "        \n",
    "        # distrebutions from & to\n",
    "        self.initial = from_dist\n",
    "        self.real    = to_dist\n",
    "        \n",
    "        # batch\n",
    "        self.batch_size = batch_size\n",
    "            \n",
    "        # epochs    \n",
    "        self.n_epochs=n_epochs\n",
    "\n",
    "        # helper for loss\n",
    "        self.ones = torch.ones((batch_size, 1))\n",
    "        self.zeros = torch.zeros((batch_size, 1))\n",
    "        \n",
    "        \n",
    "    def sample_gen(self,num=1000):\n",
    "        with torch.no_grad():\n",
    "                return self.generator(self.initial((num,1)))\n",
    "     \n",
    "    \n",
    "    def train_step(self):\n",
    "        loss_discriminator = self.train_step_discriminator()\n",
    "        loss_generator = self.train_step_generator()\n",
    "        return loss_generator,loss_discriminator\n",
    "   \n",
    "    \n",
    "    def train_step_generator(self):\n",
    "        \n",
    "        self.generator.zero_grad()\n",
    "        \n",
    "        # gen\n",
    "        sample = self.initial((self.batch_size,1))\n",
    "        gen = self.generator(sample)\n",
    "        cls = self.discriminator(gen)\n",
    "        \n",
    "        # adjusting the gen's Ws in order to fool the discriminator\n",
    "        loss = self.criterion(cls,self.ones)\n",
    "        loss.backward()\n",
    "        self.optim_generator.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    \n",
    "    def train_step_discriminator(self):\n",
    "        \n",
    "        self.discriminator.zero_grad()\n",
    "        \n",
    "        # real\n",
    "        sample = self.real((self.batch_size,1))\n",
    "        pred_real = self.discriminator(sample)\n",
    "        loss_real = self.criterion(pred_real,self.ones)\n",
    "        \n",
    "        # gen\n",
    "        sample = self.initial((self.batch_size,1))\n",
    "        with torch.no_grad():\n",
    "            gen = self.generator(sample)\n",
    "        pred_fake = self.discriminator(gen)\n",
    "        loss_fake = self.criterion(pred_fake,self.zeros)\n",
    "        \n",
    "        # combine losses\n",
    "        loss = (loss_real + loss_fake)/2\n",
    "        loss.backward()\n",
    "        self.optim_discriminator.step()\n",
    "        return loss_real.item(),loss_fake.item()\n",
    "\n",
    "    \n",
    "    def train(self,verbose_instead_of_animation=False):\n",
    "        self.loss_g, self.loss_d_real, self.loss_d_fake = [], [], []\n",
    "\n",
    "        self.samples = []\n",
    "        start = time()\n",
    "        for epoch in tqdm(range(self.n_epochs)):\n",
    "            \n",
    "            loss_g_running, loss_d_real_running, loss_d_fake_running = 0, 0, 0\n",
    "            \n",
    "            for batch in range(self.batch_size):\n",
    "                lg_, (ldr_, ldf_) = self.train_step()\n",
    "                loss_g_running += lg_\n",
    "                loss_d_real_running += ldr_\n",
    "                loss_d_fake_running += ldf_\n",
    "            \n",
    "            self.loss_g.append(loss_g_running / self.batch_size)\n",
    "            self.loss_d_real.append(loss_d_real_running / self.batch_size)\n",
    "            self.loss_d_fake.append(loss_d_fake_running / self.batch_size)\n",
    "            self.samples.append(self.sample_gen())\n",
    "            \n",
    "            if verbose_instead_of_animation and ((epoch+1)%100==0):\n",
    "                print(f\"Epoch {epoch+1}/{self.n_epochs} ({int(time() - start)}s):\"\n",
    "                      f\" G={self.loss_g[-1]:.3f},\"\n",
    "                      f\" Dr={self.loss_d_real[-1]:.3f},\"\n",
    "                      f\" Df={self.loss_d_fake[-1]:.3f}\")\n",
    "        \n",
    "        if not verbose_instead_of_animation:\n",
    "            self.plot_animation()\n",
    "        \n",
    "    \n",
    "    def plot(self,num=100):\n",
    "        fig, ax = plt.subplots(1,2,figsize=(18,6))\n",
    "\n",
    "        # plot Loss\n",
    "        ax[0].plot(self.loss_g,label='Generator Loss')\n",
    "        ax[0].plot(self.loss_d_real,label='Discriminator Loss on Real data')\n",
    "        ax[0].plot(self.loss_d_fake,label='Discriminator Loss on Fake data')\n",
    "        ax[0].set_title('Loss')\n",
    "        ax[0].legend()\n",
    "\n",
    "        # plot distribution\n",
    "        arr = self.sample_gen(num=num).detach().numpy()\n",
    "        sns.distplot(self.real(num),label='Real',hist_kws=dict(alpha=1),ax=ax[1])\n",
    "        sns.distplot(arr,label='GAN',hist_kws=dict(alpha=0.5),ax=ax[1])\n",
    "        ax[1].set_title('Distributions')\n",
    "        ax[1].legend()\n",
    "        \n",
    "    def plot_animation(self):\n",
    "        fig, ax = plt.subplots(1,2,figsize=(18,6))\n",
    "        \n",
    "        # Plot Loss\n",
    "        l, = ax[0].plot([0,10],[0,2])\n",
    "        l2, = ax[0].plot([0,10],[0,2])\n",
    "        l3, = ax[0].plot([0,10],[0,2])\n",
    "        ax[0].legend(['Generator Loss','Discriminator Loss on Real data','Discriminator Loss on Fake data'])\n",
    "        ax[0].set_title('Loss')\n",
    "\n",
    "        # Plot loss - animations functions\n",
    "        t = np.arange(len(self.loss_g))\n",
    "        animate_g = lambda i: l.set_data(t[:i], self.loss_g[:i])\n",
    "        animate_d_real = lambda i: l2.set_data(t[:i], self.loss_d_real[:i])\n",
    "        animate_d_fake = lambda i: l3.set_data(t[:i], self.loss_d_fake[:i])\n",
    "        \n",
    "        # Plot distribution\n",
    "        num = len(self.samples[0])\n",
    "        real_ = self.real(num)\n",
    "        fake_ = [np.array(s) for s in self.samples]\n",
    "        \n",
    "        # Plot distribution - animation function\n",
    "        def animate_samples(i):\n",
    "            ax[1].clear();\n",
    "            sns.distplot(real_,label='Real',hist_kws=dict(alpha=1),ax=ax[1])\n",
    "            sns.distplot(fake_[i],label='GAN',hist_kws=dict(alpha=0.5),ax=ax[1])\n",
    "            ax[1].legend()\n",
    "            ax[1].set_title('Distributions')\n",
    "\n",
    "            \n",
    "        # Plot each epoch - Loss & distribution\n",
    "        for i in range(self.n_epochs):\n",
    "            # call animation functions\n",
    "            animate_g(i)\n",
    "            animate_d_real(i)\n",
    "            animate_d_fake(i)\n",
    "            animate_samples(i)\n",
    "            \n",
    "            # clear output anf dislay figure\n",
    "            clear_output(wait=True)\n",
    "            display(fig)\n",
    "            \n",
    "            # Plot Loss - scale the plot by new values\n",
    "            ax[0].relim()\n",
    "            ax[0].autoscale_view(True,True)\n",
    "            \n",
    "            # Plot distribution - scale the plot by new values\n",
    "            ax[1].relim()\n",
    "            ax[1].autoscale_view(True,True)\n",
    "            \n",
    "            # show entire plot\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 600/600 [00:15<00:00, 39.80it/s]"
     ]
    }
   ],
   "source": [
    "gan = GAN_distribution()\n",
    "gan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(gan.sample_gen())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
